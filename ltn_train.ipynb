{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simple Combinatorial Complex Convolutional Neural Network for Mesh Classification.\n",
    "\n",
    "We create and train a mesh classification high order attentional neural network operating over combinatorial complexes. \n",
    "\n",
    "## The Neural Network:\n",
    "\n",
    "The neural network is composed of a sequence of identical convolutional layers for a dimension two combinatorial complex, a final fully connected layer embedding the features into a common space, and a final transformation to a vector with probabilities for each class. Each attention layer is composed of two levels. In both levels, messages computed for the cells of identical dimension are aggregated using a sum operation. All the messages are computed using the attention mechanisms for squared and non-squared neighborhoods presented in [Definitions 31, 32, and 33, Hajij et. al : Topological Deep Learning: Going Beyond Graph Data (2023)](https://arxiv.org/pdf/2206.00606.pdf).\n",
    "\n",
    "\n",
    "## The Task:\n",
    "\n",
    "We train this model to perform entire mesh classification on [`SHREC 2016` from the ShapeNet Dataset](http://shapenet.cs.stanford.edu/shrec16/). This dataset contains 480 3D mesh samples belonging to 30 distinct classes and represented as simplicial complexes.\n",
    "\n",
    "Each mesh contains a set of vertices, edges, and faces. Each of the latter entities have a set of features associated to them:\n",
    "\n",
    "- Node features $v \\in \\mathbb{R}^6$ defined as the direct sum of the following features:\n",
    "    - Position $p_v \\in \\mathbb{R}^3$ coordinates.\n",
    "    - Normal $n_v \\in \\mathbb{R}^3$ coordinates.\n",
    "- Edge features $e \\in \\mathbb{R}^{10}$ defined as the direct sum of the following features:\n",
    "    - Dihedral angle $\\phi \\in \\mathbb{R}$.\n",
    "    - Edge span $l \\in \\mathbb{R}$.\n",
    "    - 2 edge angle in the triangle that $\\theta_e \\in \\mathbb{R}^2$.\n",
    "    - 6 edge ratios $r \\in \\mathbb{R}^6$.\n",
    "- Face features\n",
    "    - Face area $a \\in \\mathbb{R}$.\n",
    "    - Face normal $n_f \\in \\mathbb{R}^3$.\n",
    "    - 3 face angles $\\theta_f \\in \\mathbb{R}^3$.\n",
    "\n",
    "We lift the simplicial complexes representing each mesh to a topologically equivalent combinatorial complex representation.\n",
    "\n",
    "The task is to predict the class that a certain mesh belongs to, given its combinatorial complex representation. For this purpose we implement the Higher Order Attention Model for Mesh Classification first introduced in [Hajij et. al : Topological Deep Learning: Going Beyond Graph Data (2023)](https://arxiv.org/pdf/2206.00606.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:49:28.758850145Z",
     "start_time": "2023-08-24T06:49:24.987368726Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import toponetx\n",
    "# from toponetx.datasets.mesh import shrec_16\n",
    "import toponetx.datasets as datasets\n",
    "\n",
    "# from toponetx.datasets import mesh\n",
    "# import toponetx.datasets.mesh as mesh\n",
    "# from toponetx.datasets.mesh import shrec_16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from topomodelx.nn.combinatorial.hmc_layer import HMCLayer\n",
    "# from topomodelx.nn.combinatorial.hmc import HMC\n",
    "from typing import Tuple, List\n",
    "from topomodelx.base import Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "We first create a class for the SHREC 2016 dataset. This class will be used to load the data and create the necessary neighborhood matrices for each combinatorial complex in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:49:28.810114967Z",
     "start_time": "2023-08-24T06:49:28.753170327Z"
    }
   },
   "outputs": [],
   "source": [
    "class SHRECDataset(Dataset):\n",
    "    \"\"\"Class for the SHREC 2016 dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : npz file\n",
    "        npz file containing the SHREC 2016 data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.complexes = [cc.to_combinatorial_complex() for cc in data[\"complexes\"]]\n",
    "        self.x_0 = data[\"node_feat\"]\n",
    "        self.x_1 = data[\"edge_feat\"]\n",
    "        self.x_2 = data[\"face_feat\"]\n",
    "        self.y = data[\"label\"]\n",
    "        self.a0, self.a1, self.coa2, self.b1, self.b2 = self._get_neighborhood_matrix()\n",
    "\n",
    "    def _get_neighborhood_matrix(self):\n",
    "        \"\"\"Neighborhood matrices for each combinatorial complex in the dataset.\n",
    "\n",
    "        Following the Higher Order Attention Model for Mesh Classification message passing scheme, this method computes the necessary neighborhood matrices\n",
    "        for each combinatorial complex in the dataset. This method computes:\n",
    "\n",
    "        - Adjacency matrices for each 0-cell in the dataset.\n",
    "        - Adjacency matrices for each 1-cell in the dataset.\n",
    "        - Coadjacency matrices for each 2-cell in the dataset.\n",
    "        - Incidence matrices from 1-cells to 0-cells for each 1-cell in the dataset.\n",
    "        - Incidence matrices from 2-cells to 1-cells for each 2-cell in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a0 : list of torch.sparse.FloatTensor\n",
    "            Adjacency matrices for each 0-cell in the dataset.\n",
    "        a1 : list of torch.sparse.FloatTensor\n",
    "            Adjacency matrices for each 1-cell in the dataset.\n",
    "        coa2 : list of torch.sparse.FloatTensor\n",
    "            Coadjacency matrices for each 2-cell in the dataset.\n",
    "        b1 : list of torch.sparse.FloatTensor\n",
    "            Incidence matrices from 1-cells to 0-cells for each 1-cell in the dataset.\n",
    "        b2 : list of torch.sparse.FloatTensor\n",
    "            Incidence matrices from 2-cells to 1-cells for each 2-cell in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        a0 = []\n",
    "        a1 = []\n",
    "        coa2 = []\n",
    "        b1 = []\n",
    "        b2 = []\n",
    "\n",
    "        for cc in self.complexes:\n",
    "            a0.append(torch.from_numpy(cc.adjacency_matrix(0, 1).todense()).to_sparse())\n",
    "            a1.append(torch.from_numpy(cc.adjacency_matrix(1, 2).todense()).to_sparse())\n",
    "\n",
    "            B = cc.incidence_matrix(rank=1, to_rank=2)\n",
    "            A = B.T @ B\n",
    "            A.setdiag(0)\n",
    "            coa2.append(torch.from_numpy(A.todense()).to_sparse())\n",
    "\n",
    "            b1.append(torch.from_numpy(cc.incidence_matrix(0, 1).todense()).to_sparse())\n",
    "            b2.append(torch.from_numpy(cc.incidence_matrix(1, 2).todense()).to_sparse())\n",
    "\n",
    "        return a0, a1, coa2, b1, b2\n",
    "\n",
    "    def num_classes(self):\n",
    "        \"\"\"Returns the number of classes in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of classes in the dataset.\n",
    "        \"\"\"\n",
    "        return len(np.unique(self.y))\n",
    "\n",
    "    def channels_dim(self):\n",
    "        \"\"\"Returns the number of channels for each input signal.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of int\n",
    "            Number of channels for each input signal.\n",
    "        \"\"\"\n",
    "        return [self.x_0[0].shape[1], self.x_1[0].shape[1], self.x_2[0].shape[1]]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of elements in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of elements in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.complexes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the idx-th element in the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the element to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            Tuple containing the idx-th element in the dataset, including the input signals on nodes, edges and faces, the neighborhood matrices and the label.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.x_0[idx],\n",
    "            self.x_1[idx],\n",
    "            self.x_2[idx],\n",
    "            self.a0[idx],\n",
    "            self.a1[idx],\n",
    "            self.coa2[idx],\n",
    "            self.b1[idx],\n",
    "            self.b2[idx],\n",
    "            self.y[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:49:29.818662234Z",
     "start_time": "2023-08-24T06:49:28.767068605Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading shrec 16 full dataset...\n",
      "\n",
      "done!\n",
      "Loading shrec 16 full dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec_training, shrec_testing = datasets.shrec_16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:50:47.902394227Z",
     "start_time": "2023-08-24T06:49:31.440777885Z"
    }
   },
   "outputs": [],
   "source": [
    "training_dataset = SHRECDataset(shrec_training)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:55:26.795250682Z",
     "start_time": "2023-08-24T06:55:02.115686839Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_dataset = SHRECDataset(shrec_testing)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T09:12:40.935701Z",
     "start_time": "2023-06-29T09:12:40.933077Z"
    }
   },
   "source": [
    "# Create the Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The task is to classify the meshes into their corresponding classes. To address this, we employ the Higher Order Attention Network Model for Mesh Classification, as outlined in the article [Higher Order Attention Networks](https://www.researchgate.net/publication/361022512_Higher-Order_Attention_Networks). This model integrates a hierarchical and attention-based message passing scheme as per the article's descriptions. In addition, the model utilizes a final sum pooling layer which effectively maps the nodal, edge, and face features of the meshes into a shared N-dimensional Euclidean space, where N represents the number of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:55:26.803747542Z",
     "start_time": "2023-08-24T06:55:26.801465118Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LTN_Classification(nn.Module): \n",
    "    def __init__(self, in_ch_v, in_ch_e, in_ch_f, out_dim, num_classes, activation=None,device=None):\n",
    "        super(LTN_Classification, self).__init__()\n",
    "\n",
    "        \n",
    "        self.conv_v2v = Conv(in_channels=in_ch_v, out_channels=out_dim)\n",
    "        self.conv_e2e = Conv(in_channels=in_ch_e, out_channels=out_dim)\n",
    "        self.conv_f2f = Conv(in_channels=in_ch_f, out_channels=out_dim)\n",
    "\n",
    "        self.conv_v2e = Conv(in_channels=out_dim, out_channels=out_dim)\n",
    "        self.con_e2f = Conv(in_channels=out_dim, out_channels=out_dim)\n",
    "\n",
    "\n",
    "        self.lin = nn.Linear(2 * out_dim, num_classes) \n",
    "        # self.lin2 = nn.Linear(out_dim, num_classes) \n",
    "        # self.lin3 = nn.Linear(out_dim, num_classes) \n",
    "\n",
    "        self.act = activation\n",
    "\n",
    "\n",
    "    def forward(self, xv, xe, xf,\n",
    "                gv2v, ge2e, gf2f, gv2e, ge2f ):\n",
    "        zv1 = self.conv_v2v(xv, gv2v)\n",
    "        ze1 = self.conv_e2e(xe, ge2e)\n",
    "        zf1 = self.conv_f2f(xf, gf2f)\n",
    "        \n",
    "        zv1 = self.act(zv1)\n",
    "        ze1 = self.act(ze1)\n",
    "        zf1 = self.act(zf1)\n",
    "\n",
    "        ze2 = self.conv_v2e(zv1, gv2e, ze1)\n",
    "        zf2 = self.con_e2f(ze1, ge2f, zf1)\n",
    "\n",
    "\n",
    "        \n",
    "        zf2 = torch.mean(zf2, dim=0)\n",
    "        zf2 = self.act(zf2)\n",
    "        ze2 = torch.mean(zf2, dim=0)\n",
    "        ze2 = self.act(ze2)\n",
    "\n",
    "        print(f'zf2: {zf2.shape} ze2: {ze2.shape}')\n",
    "        out = torch.cat([zf2, ze2], dim=0)\n",
    "        out = self.lin(out)\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T10:21:11.467775Z",
     "start_time": "2023-06-29T10:21:11.463344Z"
    }
   },
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We create the trainer class. The model is trained using the Adam optimizer and the Cross Entropy Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:55:26.814069014Z",
     "start_time": "2023-08-24T06:55:26.809087142Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer for the HOANMeshClassifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to train.\n",
    "    training_dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader for the training set.\n",
    "    testing_dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader for the testing set.\n",
    "    learning_rate : float\n",
    "        The learning rate for the Adam optimizer.\n",
    "    device : torch.device\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, training_dataloader, testing_dataloader, learning_rate, device\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.testing_dataloader = testing_dataloader\n",
    "        self.device = device\n",
    "        self.crit = torch.nn.CrossEntropyLoss()\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def _to_device(self, x):\n",
    "        \"\"\"Converts tensors to the correct type and moves them to the device.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : List[torch.Tensor]\n",
    "            List of tensors to convert.\n",
    "        Returns\n",
    "        -------\n",
    "        List[torch.Tensor]\n",
    "            List of converted tensors to float type and moved to the device.\n",
    "        \"\"\"\n",
    "\n",
    "        return [el[0].float().to(self.device) for el in x]\n",
    "\n",
    "    def train(self, num_epochs=500, test_interval=25):\n",
    "        \"\"\"Trains the model for the specified number of epochs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs : int\n",
    "            Number of epochs to train.\n",
    "        test_interval : int\n",
    "            Interval between testing epochs.\n",
    "        \"\"\"\n",
    "        for epoch_i in range(1, num_epochs + 1):\n",
    "            training_accuracy, epoch_loss = self._train_epoch()\n",
    "            print(\n",
    "                f\"Epoch: {epoch_i} loss: {epoch_loss:.4f} Train_acc: {training_accuracy:.4f}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            if epoch_i % test_interval == 0:\n",
    "                test_accuracy = self.validate()\n",
    "                print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        \"\"\"Trains the model for one epoch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        training_accuracy : float\n",
    "            The mean training accuracy for the epoch.\n",
    "        epoch_loss : float\n",
    "            The mean loss for the epoch.\n",
    "        \"\"\"\n",
    "        training_samples = len(self.training_dataloader.dataset)\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        self.model.train()\n",
    "        for sample in self.training_dataloader:\n",
    "            (\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                adjacency_0,\n",
    "                adjacency_1,\n",
    "                coadjacency_2,\n",
    "                incidence_1,\n",
    "                incidence_2,\n",
    "            ) = self._to_device(sample[:-1])\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            y_hat = self.model.forward(\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                adjacency_0,\n",
    "                adjacency_1,\n",
    "                coadjacency_2,\n",
    "                incidence_1.T,\n",
    "                incidence_2.T,\n",
    "            )\n",
    "\n",
    "            y = sample[-1][0].long().to(self.device)\n",
    "            total_loss += self._compute_loss_and_update(y_hat, y)\n",
    "            correct += (y_hat.argmax() == y).sum().item()\n",
    "\n",
    "        training_accuracy = correct / training_samples\n",
    "        epoch_loss = total_loss / training_samples\n",
    "\n",
    "        return training_accuracy, epoch_loss\n",
    "\n",
    "    def _compute_loss_and_update(self, y_hat, y):\n",
    "        \"\"\"Computes the loss, performs backpropagation, and updates the model's parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hat : torch.Tensor\n",
    "            The output of the model.\n",
    "        y : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = self.crit(y_hat, y)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validates the model using the testing dataloader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        test_accuracy : float\n",
    "            The mean testing accuracy.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        self.model.eval()\n",
    "        test_samples = len(self.testing_dataloader.dataset)\n",
    "        with torch.no_grad():\n",
    "            for sample in self.testing_dataloader:\n",
    "                (\n",
    "                    x_0,\n",
    "                    x_1,\n",
    "                    x_2,\n",
    "                    adjacency_0,\n",
    "                    adjacency_1,\n",
    "                    coadjacency_2,\n",
    "                    incidence_1,\n",
    "                    incidence_2,\n",
    "                ) = self._to_device(sample[:-1])\n",
    "\n",
    "                y_hat = self.model(\n",
    "                    x_0,\n",
    "                    x_1,\n",
    "                    x_2,\n",
    "                    adjacency_0,\n",
    "                    adjacency_1,\n",
    "                    coadjacency_2,\n",
    "                    incidence_1.T,\n",
    "                    incidence_2.T,\n",
    "                )\n",
    "                y = sample[-1][0].long().to(self.device)\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "            test_accuracy = correct / test_samples\n",
    "            return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We define the parameters for the model. We use softmax activation for the attention layers. Moreover, we use relu activation for the update and the aggregation steps. We set the negative slope parameter for the Leaky ReLU activation to 0.2. We only use one higher order attention layer as it already achieves almost perfect test accuracy, although more layers could be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T06:55:28.220563261Z",
     "start_time": "2023-08-24T06:55:26.817957236Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "in_channels = training_dataset.channels_dim()\n",
    "intermediate_channels = [60, 60, 60]\n",
    "final_channels = [60, 60, 60]\n",
    "\n",
    "channels_per_layer = [[in_channels, intermediate_channels, final_channels]]\n",
    "# defube HOAN mesh classifier\n",
    "# model = Cxn_att_classification(\n",
    "#     channels_per_layer, negative_slope=0.2, num_classes=training_dataset.num_classes()\n",
    "# )\n",
    "model = LTN_Classification(in_ch_v=in_channels[0],in_ch_e=in_channels[1], in_ch_f=in_channels[2], out_dim=60, num_classes=training_dataset.num_classes(), activation=nn.LeakyReLU(negative_slope=0.2))\n",
    "# If GPU's are available, we will make use of them. Otherwise, this will run on CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {device}\")\n",
    "trainer = Trainer(model, training_dataloader, testing_dataloader, 0.001, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the HoanMeshClassifier using low amount of epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T07:01:54.496249166Z",
     "start_time": "2023-08-24T07:00:40.556296097Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 1) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32m/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Trains the model for the specified number of epochs.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m    Interval between testing epochs.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     training_accuracy, epoch_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch_i\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Train_acc: \u001b[39m\u001b[39m{\u001b[39;00mtraining_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch_i \u001b[39m%\u001b[39m test_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     x_0,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     x_1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     incidence_2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_device(sample[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     x_0,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     x_1,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     x_2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     adjacency_0,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     adjacency_1,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     coadjacency_2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     incidence_1\u001b[39m.\u001b[39;49mT,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     incidence_2\u001b[39m.\u001b[39;49mT,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m y \u001b[39m=\u001b[39m sample[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_loss_and_update(y_hat, y)\n",
      "\u001b[1;32m/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m ze2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(zf2, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m ze2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(ze2)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(torch\u001b[39m.\u001b[39;49mcat([zf2, ze2], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btopgraph.cs.purdue.edu/homes/mukher26/scratch1/tdl-demo/cxin_train.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 1) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting the model train for longer, we can see that the model achieves an outstanding performance on both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T07:13:58.198454432Z",
     "start_time": "2023-08-24T07:01:54.497740412Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.4085 Train_acc: 0.0271\n",
      "Epoch: 2 loss: 3.4083 Train_acc: 0.0229\n",
      "Epoch: 3 loss: 3.4081 Train_acc: 0.0229\n",
      "Epoch: 4 loss: 3.4083 Train_acc: 0.0146\n",
      "Epoch: 5 loss: 3.4062 Train_acc: 0.0187\n",
      "Epoch: 6 loss: 3.4062 Train_acc: 0.0271\n",
      "Epoch: 7 loss: 3.3952 Train_acc: 0.0312\n",
      "Epoch: 8 loss: 3.2418 Train_acc: 0.0604\n",
      "Epoch: 9 loss: 3.1035 Train_acc: 0.0813\n",
      "Epoch: 10 loss: 3.0070 Train_acc: 0.0771\n",
      "Test_acc: 0.0833\n",
      "Epoch: 11 loss: 2.9556 Train_acc: 0.0938\n",
      "Epoch: 12 loss: 2.9448 Train_acc: 0.0958\n",
      "Epoch: 13 loss: 2.8683 Train_acc: 0.1271\n",
      "Epoch: 14 loss: 2.8592 Train_acc: 0.1208\n",
      "Epoch: 15 loss: 2.8816 Train_acc: 0.1417\n",
      "Epoch: 16 loss: 2.7917 Train_acc: 0.1417\n",
      "Epoch: 17 loss: 2.7517 Train_acc: 0.1729\n",
      "Epoch: 18 loss: 2.7197 Train_acc: 0.1375\n",
      "Epoch: 19 loss: 2.6759 Train_acc: 0.1667\n",
      "Epoch: 20 loss: 2.5774 Train_acc: 0.1667\n",
      "Test_acc: 0.1417\n",
      "Epoch: 21 loss: 2.4718 Train_acc: 0.2208\n",
      "Epoch: 22 loss: 2.4021 Train_acc: 0.2313\n",
      "Epoch: 23 loss: 2.3240 Train_acc: 0.2708\n",
      "Epoch: 24 loss: 2.3141 Train_acc: 0.2750\n",
      "Epoch: 25 loss: 2.2089 Train_acc: 0.2896\n",
      "Epoch: 26 loss: 2.2364 Train_acc: 0.2812\n",
      "Epoch: 27 loss: 2.1958 Train_acc: 0.2979\n",
      "Epoch: 28 loss: 2.1104 Train_acc: 0.2958\n",
      "Epoch: 29 loss: 2.0956 Train_acc: 0.3417\n",
      "Epoch: 30 loss: 2.1176 Train_acc: 0.3354\n",
      "Test_acc: 0.1833\n",
      "Epoch: 31 loss: 2.0158 Train_acc: 0.3458\n",
      "Epoch: 32 loss: 2.0328 Train_acc: 0.3500\n",
      "Epoch: 33 loss: 2.0025 Train_acc: 0.3333\n",
      "Epoch: 34 loss: 1.9389 Train_acc: 0.3500\n",
      "Epoch: 35 loss: 1.8971 Train_acc: 0.3500\n",
      "Epoch: 36 loss: 2.0561 Train_acc: 0.3646\n",
      "Epoch: 37 loss: 1.8783 Train_acc: 0.4021\n",
      "Epoch: 38 loss: 1.8479 Train_acc: 0.3688\n",
      "Epoch: 39 loss: 1.8852 Train_acc: 0.3646\n",
      "Epoch: 40 loss: 1.8360 Train_acc: 0.4104\n",
      "Test_acc: 0.3250\n",
      "Epoch: 41 loss: 1.7523 Train_acc: 0.3958\n",
      "Epoch: 42 loss: 1.8885 Train_acc: 0.3812\n",
      "Epoch: 43 loss: 1.7638 Train_acc: 0.4208\n",
      "Epoch: 44 loss: 1.6907 Train_acc: 0.4292\n",
      "Epoch: 45 loss: 1.7342 Train_acc: 0.4104\n",
      "Epoch: 46 loss: 1.7054 Train_acc: 0.4208\n",
      "Epoch: 47 loss: 1.5863 Train_acc: 0.4437\n",
      "Epoch: 48 loss: 1.5585 Train_acc: 0.4479\n",
      "Epoch: 49 loss: 1.5163 Train_acc: 0.4667\n",
      "Epoch: 50 loss: 1.6265 Train_acc: 0.4813\n",
      "Test_acc: 0.2333\n",
      "Epoch: 51 loss: 1.5700 Train_acc: 0.4646\n",
      "Epoch: 52 loss: 1.4413 Train_acc: 0.5042\n",
      "Epoch: 53 loss: 1.4119 Train_acc: 0.5292\n",
      "Epoch: 54 loss: 1.4267 Train_acc: 0.5208\n",
      "Epoch: 55 loss: 1.3498 Train_acc: 0.5125\n",
      "Epoch: 56 loss: 1.2774 Train_acc: 0.5563\n",
      "Epoch: 57 loss: 1.2583 Train_acc: 0.5500\n",
      "Epoch: 58 loss: 1.2208 Train_acc: 0.5792\n",
      "Epoch: 59 loss: 1.1934 Train_acc: 0.5833\n",
      "Epoch: 60 loss: 1.1999 Train_acc: 0.5687\n",
      "Test_acc: 0.3583\n",
      "Epoch: 61 loss: 1.1875 Train_acc: 0.5938\n",
      "Epoch: 62 loss: 1.1123 Train_acc: 0.6208\n",
      "Epoch: 63 loss: 1.0502 Train_acc: 0.6208\n",
      "Epoch: 64 loss: 1.0535 Train_acc: 0.6417\n",
      "Epoch: 65 loss: 1.0864 Train_acc: 0.6146\n",
      "Epoch: 66 loss: 1.0299 Train_acc: 0.6562\n",
      "Epoch: 67 loss: 1.0125 Train_acc: 0.6125\n",
      "Epoch: 68 loss: 0.9080 Train_acc: 0.6729\n",
      "Epoch: 69 loss: 0.9323 Train_acc: 0.6813\n",
      "Epoch: 70 loss: 0.9714 Train_acc: 0.6438\n",
      "Test_acc: 0.4167\n",
      "Epoch: 71 loss: 0.9431 Train_acc: 0.6792\n",
      "Epoch: 72 loss: 0.8804 Train_acc: 0.6792\n",
      "Epoch: 73 loss: 0.9047 Train_acc: 0.6875\n",
      "Epoch: 74 loss: 0.8503 Train_acc: 0.7208\n",
      "Epoch: 75 loss: 0.9519 Train_acc: 0.6750\n",
      "Epoch: 76 loss: 0.8241 Train_acc: 0.7167\n",
      "Epoch: 77 loss: 0.8423 Train_acc: 0.6958\n",
      "Epoch: 78 loss: 0.7921 Train_acc: 0.7125\n",
      "Epoch: 79 loss: 0.8623 Train_acc: 0.6854\n",
      "Epoch: 80 loss: 0.8159 Train_acc: 0.6979\n",
      "Test_acc: 0.4667\n",
      "Epoch: 81 loss: 0.7509 Train_acc: 0.7229\n",
      "Epoch: 82 loss: 0.7977 Train_acc: 0.7208\n",
      "Epoch: 83 loss: 0.7747 Train_acc: 0.7500\n",
      "Epoch: 84 loss: 0.8208 Train_acc: 0.7250\n",
      "Epoch: 85 loss: 0.7648 Train_acc: 0.7417\n",
      "Epoch: 86 loss: 0.7452 Train_acc: 0.7333\n",
      "Epoch: 87 loss: 0.7605 Train_acc: 0.7438\n",
      "Epoch: 88 loss: 0.7636 Train_acc: 0.7458\n",
      "Epoch: 89 loss: 0.6652 Train_acc: 0.7583\n",
      "Epoch: 90 loss: 0.6807 Train_acc: 0.7646\n",
      "Test_acc: 0.4000\n",
      "Epoch: 91 loss: 0.6708 Train_acc: 0.7500\n",
      "Epoch: 92 loss: 0.6252 Train_acc: 0.7688\n",
      "Epoch: 93 loss: 0.6934 Train_acc: 0.7667\n",
      "Epoch: 94 loss: 0.6938 Train_acc: 0.7521\n",
      "Epoch: 95 loss: 0.5920 Train_acc: 0.7875\n",
      "Epoch: 96 loss: 0.6008 Train_acc: 0.8063\n",
      "Epoch: 97 loss: 0.6560 Train_acc: 0.7917\n",
      "Epoch: 98 loss: 0.6971 Train_acc: 0.7479\n",
      "Epoch: 99 loss: 0.6790 Train_acc: 0.7667\n",
      "Epoch: 100 loss: 0.6165 Train_acc: 0.7771\n",
      "Test_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=100, test_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
